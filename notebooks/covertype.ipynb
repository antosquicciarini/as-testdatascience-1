{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a820c8f",
   "metadata": {},
   "source": [
    "# Problem Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a57e7",
   "metadata": {},
   "source": [
    "## Dataset Description & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3151f4c",
   "metadata": {},
   "source": [
    "### Forest CoverType Dataset\n",
    "\n",
    "This dataset contains **581,012** observations of **30 × 30 m** forest patches from Roosevelt National Forest in northern Colorado. Each sample is described by **54 cartographic features** (elevation, slope, aspects, hydrology distances, hillshade at different times, plus binary soil- and wilderness-area indicators). The goal is to classify each sample into one of **seven forest cover types** (e.g. Spruce/Fir, Lodgepole Pine, Ponderosa Pine, etc.), based purely on map-derived data—no imagery involved. The data were collected via U.S. Forest Service and USGS systems, include no missing values, and cover diverse ecological zones. Originally published in 1998 by researchers at Colorado State University (Blackard & Dean), this dataset has since become a standard benchmark for multiclass forest-type classification tasks.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/dataset/31/covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import umap \n",
    "import warnings\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from models.train_model import train_models\n",
    "from models.predict_model import evaluate_models\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "RANDOM_SEED = 18\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Dataset Uploading\n",
    "from data.make_dataset import load_dataset\n",
    "df, target_col, quantitative_cols, binary_cols = load_dataset()\n",
    "\n",
    "\n",
    "# --- Output summaries ---\n",
    "print(f\"✅ Quantitative features ({len(quantitative_cols)}):\\n\", quantitative_cols)\n",
    "print(f\"\\n✅ Binary features ({len(binary_cols)}):\\n\", binary_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cb5ed",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a6a9c",
   "metadata": {},
   "source": [
    "## General Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Dimensions:\", df.shape)\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fe2ee",
   "metadata": {},
   "source": [
    "## Missing values and duplicates Check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "if missing_values.empty:\n",
    "    print(\"✅ No missing values detected in the dataset.\")\n",
    "else:\n",
    "    print(\"⚠️ Missing values found:\")\n",
    "    display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "if duplicate_count == 0:\n",
    "    print(\"✅ No duplicate rows detected in the dataset.\")\n",
    "else:\n",
    "    print(f\"⚠️ {duplicate_count} duplicate rows found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be6610",
   "metadata": {},
   "source": [
    "## Class distribution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27246c2",
   "metadata": {},
   "source": [
    "As shown in the following images, the dataset suffers from significant class imbalance, which must be taken into account in the classification phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd74b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution counts\n",
    "print(\"\\nTarget class distribution:\")\n",
    "print(df[target_col].value_counts().sort_index())\n",
    "\n",
    "sns.countplot(data=df, x=target_col, hue=target_col, palette=\"viridis\", legend=False)\n",
    "plt.title(\"Target Class Distribution (Cover Type)\")\n",
    "plt.xlabel(\"Forest Cover Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Compute class counts and percentages\n",
    "class_counts = df[target_col].value_counts().sort_index()\n",
    "class_percent = class_counts / len(df) * 100\n",
    "\n",
    "# Prepare DataFrame for barplot\n",
    "bar_df = pd.DataFrame({\n",
    "    \"CoverType\": class_counts.index,\n",
    "    \"Percentage\": class_percent.values\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=bar_df, x=\"CoverType\", y=\"Percentage\", hue=\"CoverType\", palette=\"mako\", legend=False)\n",
    "plt.title(\"Percentage of Samples per Cover Type\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Cover Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea8731",
   "metadata": {},
   "source": [
    "### Descriptive Statistics Summary (Excluding Binary Columns)\n",
    "\n",
    "This table summarizes the central tendency and dispersion of the main continuous variables in the Forest Cover Type dataset:\n",
    "\n",
    "- **Elevation**: Ranges from **1859m to 3858m** with a mean of ~2960m, indicating substantial terrain variation across forested mountain regions.\n",
    "- **Aspect**: Uniformly spans from **0° to 360°**, showing a wide distribution of terrain orientations. The mean (~156°) suggests a slight dominance of SE-facing slopes.\n",
    "- **Slope**: Most values are modest, with **median = 13°** and max at **66°**.\n",
    "\n",
    "- **Horizontal & Vertical Distance to Hydrology**:\n",
    "  - Horizontal: Average ~269m from water, but some points lie over **1.3 km** away.\n",
    "  - Vertical: Ranges from **-173m to +601m**, with negative values representing points uphill from water sources.\n",
    "\n",
    "- **Horizontal Distance to Roadways**: Mean ~2.3 km; ranges up to **7.1 km**, suggesting many remote points.\n",
    "\n",
    "- **Hillshade (9am, Noon, 3pm)**:\n",
    "  - Values follow the expected sun cycle with **higher illumination around noon**.\n",
    "\n",
    "- **Horizontal Distance to Fire Points**: Wide spread with a **mean of ~1980m**; some samples are very close (**0m**) and others very far (**7173m**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive Statistics (excluding binary variable columns):\")\n",
    "print(df[quantitative_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e869a",
   "metadata": {},
   "source": [
    "### Distribution of Soil Types and Wilderness Areas\n",
    "\n",
    "The first histogram shows the **relative frequency of each `Soil_Type`** variable in the dataset. It is evident that a few soil types dominate the data—most notably `Soil_Type29`, which represents nearly 20% of the samples, followed by `Soil_Type30`, `Soil_Type23`, and others. In contrast, many soil types occur very infrequently, with some appearing in less than 1% of the observations. This imbalance should be considered in downstream modeling, especially if certain soil types are predictive of forest cover.\n",
    "\n",
    "The second histogram depicts the **relative frequency of each `Wilderness_Area`**. Two classes—`Wilderness_Area1` and `Wilderness_Area3`—account for the majority of the dataset, each with more than 40% of the samples. The remaining two classes (`Wilderness_Area2` and `Wilderness_Area4`) are underrepresented. This skewed distribution may influence model performance and should be accounted for, potentially via stratified sampling or class balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a36b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Soil_Type Analysis ---\n",
    "soil_type_cols = [col for col in binary_cols if col.startswith('Soil_Type')]\n",
    "soil_type_freq = df[soil_type_cols].sum().sort_values(ascending=False) / len(df)\n",
    "\n",
    "# Convert to DataFrame to allow hue assignment\n",
    "soil_df = soil_type_freq.reset_index()\n",
    "soil_df.columns = ['Soil_Type', 'Frequency']\n",
    "\n",
    "# Plot histogram for Soil_Type (with hue to avoid FutureWarning)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=soil_df, x=\"Soil_Type\", y=\"Frequency\", hue=\"Soil_Type\", palette=\"viridis\", legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Relative Frequency\")\n",
    "plt.title(\"Relative Frequency of Each Soil_Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mutual exclusivity check\n",
    "soil_type_sum = df[soil_type_cols].sum(axis=1)\n",
    "non_exclusive_soil = (soil_type_sum != 1).sum()\n",
    "if non_exclusive_soil == 0:\n",
    "    print(\"Soil_Type columns are mutually exclusive (exactly one per row).\")\n",
    "else:\n",
    "    print(f\"{non_exclusive_soil} rows violate Soil_Type mutual exclusivity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78585179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wilderness_Area Analysis ---\n",
    "wilderness_cols = [col for col in binary_cols if col.startswith('Wilderness_Area')]\n",
    "wilderness_freq = df[wilderness_cols].sum().sort_values(ascending=False) / len(df)\n",
    "\n",
    "# Convert to DataFrame to allow hue assignment\n",
    "wilderness_df = wilderness_freq.reset_index()\n",
    "wilderness_df.columns = ['Wilderness_Area', 'Frequency']\n",
    "\n",
    "# Plot histogram for Wilderness_Area (with hue to avoid FutureWarning)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=wilderness_df, x=\"Wilderness_Area\", y=\"Frequency\", hue=\"Wilderness_Area\", palette=\"crest\", legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Relative Frequency\")\n",
    "plt.title(\"Relative Frequency of Each Wilderness_Area\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mutual exclusivity check\n",
    "wilderness_sum = df[wilderness_cols].sum(axis=1)\n",
    "non_exclusive_wilderness = (wilderness_sum != 1).sum()\n",
    "if non_exclusive_wilderness == 0:\n",
    "    print(\"Wilderness_Area columns are mutually exclusive (exactly one per row).\")\n",
    "else:\n",
    "    print(f\"{non_exclusive_wilderness} rows violate Wilderness_Area mutual exclusivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2753c3b",
   "metadata": {},
   "source": [
    "## Feature Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d83e2b",
   "metadata": {},
   "source": [
    "\n",
    "The correlation matrix shows that most quantitative features exhibit weak to moderate linear relationships with one another. Notably, `Hillshade_9am` and `Hillshade_3pm` are strongly correlated, likely due to similar terrain shading behavior throughout the day. Other features such as `Aspect` and `Slope` show little linear correlation with the rest of the dataset, suggesting they may capture more independent or nonlinear patterns.\n",
    "\n",
    "The mutual information matrix complements this by revealing **nonlinear dependencies** not captured by correlation. For example, `Hillshade_9am`, `Hillshade_Noon`, and `Aspect` display higher mutual information with other features, highlighting their potential relevance in models capable of learning nonlinear interactions (e.g., tree-based models or ensemble methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6b068",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb47797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for quantitative features\n",
    "corr = df[quantitative_cols].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0, square=True, linewidths=0.5, annot=False)\n",
    "plt.title(\"Correlation Matrix of Quantitative Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information between all pairs (MI is not symmetric like correlation, so we fake a matrix)\n",
    "mi_matrix = pd.DataFrame(index=quantitative_cols, columns=quantitative_cols)\n",
    "\n",
    "for col_x in quantitative_cols:\n",
    "    X = df[quantitative_cols].copy()\n",
    "    y = df[col_x]\n",
    "    X = X.drop(columns=[col_x])\n",
    "    \n",
    "    mi = mutual_info_regression(X, y, discrete_features=False, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for i, col_y in enumerate(X.columns):\n",
    "        mi_matrix.loc[col_x, col_y] = mi[i]\n",
    "\n",
    "# Convert to float for plotting\n",
    "mi_matrix = mi_matrix.astype(float)\n",
    "\n",
    "# Fill diagonal with NaN (or 0, but MI with self is not informative)\n",
    "np.fill_diagonal(mi_matrix.values, np.nan)\n",
    "\n",
    "# Plot the mutual information heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(mi_matrix, cmap=\"YlGnBu\", linewidths=0.5, square=True, center=0, annot=False)\n",
    "plt.title(\"Mutual Information Matrix of Quantitative Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47038e85",
   "metadata": {},
   "source": [
    "## 📊 Feature Distribution by Cover Type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6af6b0",
   "metadata": {},
   "source": [
    "### Soil and Wilderness Area Influence on Cover Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfa563",
   "metadata": {},
   "source": [
    "The heatmaps below illustrate how the binary-encoded `Soil_Type` and `Wilderness_Area` features are distributed across the seven `Cover_Type` classes. Each cell represents the **fraction of samples** in a given class for which a particular binary variable is active (value = 1).\n",
    "\n",
    "#### 🔹 Soil_Type Distribution\n",
    "- Most soil types are **strongly associated with specific forest cover types**.\n",
    "- For example:\n",
    "  - `Soil_Type10` is highly concentrated in Cover Type 6.\n",
    "  - `Soil_Type29` and `Soil_Type27` are common in Types 2 and 3.\n",
    "  - `Soil_Type23` appears primarily in Type 4.\n",
    "- Many soil types are sparsely used or class-specific, indicating **strong categorical predictive value**.\n",
    "\n",
    "#### 🔸 Wilderness_Area Distribution\n",
    "- The four wilderness areas are **mutually exclusive** and show sharp class-specific patterns:\n",
    "  - `Wilderness_Area1` is most frequent in Types 1 and 2.\n",
    "  - `Wilderness_Area3` dominates Cover Types 5, 6, and 7.\n",
    "  - `Wilderness_Area4` is **exclusively active in Types 3–6**, and makes up 100% of Type 4.\n",
    "- These patterns suggest that **land zoning or regional boundaries** correlate strongly with forest type labels.\n",
    "\n",
    "### ✅ Summary\n",
    "Both `Soil_Type` and `Wilderness_Area` features carry **substantial discriminative power**. Their sparsity and sharp associations with certain `Cover_Type` values make them especially well-suited for **tree-based models** or any classifier that handles categorical features effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select binary columns\n",
    "soil_cols = [col for col in df.columns if col.startswith(\"Soil_Type\")]\n",
    "wilderness_cols = [col for col in df.columns if col.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "# Grouped frequency by Cover_Type (mean of 0/1 indicates percentage of rows in each group)\n",
    "soil_distribution = df.groupby(\"Cover_Type\")[soil_cols].mean()\n",
    "wilderness_distribution = df.groupby(\"Cover_Type\")[wilderness_cols].mean()\n",
    "\n",
    "# Plotting heatmap for Soil_Type\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(soil_distribution.T, cmap=\"YlGnBu\", annot=False, cbar=True)\n",
    "plt.title(\"Soil_Type Distribution by Cover_Type\")\n",
    "plt.xlabel(\"Cover_Type\")\n",
    "plt.ylabel(\"Soil_Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting heatmap for Wilderness_Area\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(wilderness_distribution.T, cmap=\"YlOrBr\", annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Wilderness_Area Distribution by Cover_Type\")\n",
    "plt.xlabel(\"Cover_Type\")\n",
    "plt.ylabel(\"Wilderness_Area\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee60914",
   "metadata": {},
   "source": [
    "### Distributions of quantitative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c429f9",
   "metadata": {},
   "source": [
    "To understand how the quantitative features relate to forest cover types, we analyzed their distributions across the seven `Cover_Type` classes using both **density plots** and **boxplots**.\n",
    "\n",
    "#### 🔹 Key Observations:\n",
    "\n",
    "- **Elevation** shows the strongest class-wise separation. For example, Class 1 and Class 7 appear at the highest elevations, while Classes 3 and 4 are concentrated at lower altitudes.\n",
    "- **Aspect** (terrain orientation) and **Slope** show more overlapping distributions across classes, with no strong class-level separation, although slight tendencies exist (e.g., Class 4 has more concentrated aspect values).\n",
    "- **Horizontal and Vertical Distance to Hydrology** both show long-tailed distributions. Though overlapping, some classes (like Class 4 and 7) are slightly shifted, indicating mild differences in terrain proximity to water sources.\n",
    "- **Hillshade features (9am, Noon, 3pm)** provide some separation — especially `Hillshade_9am`, where Class 6 appears distinct.\n",
    "- **Horizontal Distance to Roadways and Fire Points** show class-specific clusters, with Class 2 and 7 having broader or more distant ranges, possibly reflecting different land use or accessibility.\n",
    "  \n",
    "#### 🧠 Summary:\n",
    "While some features (like `Elevation`) offer strong discriminatory power between classes, others (e.g., `Aspect`, `Slope`) may contribute more subtly. This analysis reinforces the value of multivariate models, as no single feature cleanly separates all cover types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58aa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "class_labels = sorted(df['Cover_Type'].unique())\n",
    "\n",
    "# Set up plot grid\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(len(quantitative_cols) / num_cols))\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 4 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot KDEs for each class\n",
    "for i, col in enumerate(quantitative_cols):\n",
    "    for cls in class_labels:\n",
    "        subset = df[df['Cover_Type'] == cls]\n",
    "        sns.kdeplot(subset[col], ax=axes[i], label=f\"Class {cls}\", linewidth=1.5)\n",
    "    \n",
    "    axes[i].set_title(f\"Distribution of {col} by Cover_Type\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend(title=\"Cover_Type\", fontsize='small', title_fontsize='small')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f0840",
   "metadata": {},
   "source": [
    "### Boxplots to detect outlier in key feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in quantitative_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x=target_col, y=col, data=df, hue=target_col, palette=\"Set2\", legend=False)\n",
    "    plt.title(f\"{col} by Cover Type\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e18ae",
   "metadata": {},
   "source": [
    "## Data visualizazion trough dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb99e13",
   "metadata": {},
   "source": [
    "### 🌐 Dimensionality Reduction: PCA, t-SNE, and UMAP\n",
    "\n",
    "To explore the structure of the dataset in a lower-dimensional space, three dimensionality reduction techniques were applied to the quantitative features:\n",
    "\n",
    "#### 🔹 PCA (Principal Component Analysis)\n",
    "The PCA plot shows a smooth, continuous manifold, indicating that PCA captures some global structure. However, the class separation is minimal—most `Cover_Type` classes heavily overlap, suggesting that **PCA is not effective** at revealing distinct clusters in this dataset.\n",
    "\n",
    "#### 🔹 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "t-SNE produces well-separated clusters for groups of classes, with clearly defined local groupings. This indicates that t-SNE is effective at preserving local structure and highlights potential nonlinear separability among some forest cover types. However, t-SNE does not preserve global distances, so the relative positioning of clusters is not meaningful.\n",
    "\n",
    "#### 🔹 UMAP (Uniform Manifold Approximation and Projection)\n",
    "UMAP also reveals strong cluster separation while preserving both local and some global structure. For example, Classes 3, 4, and 6 form distinct, dense regions relative to the other classes, which themselves form a separate cluster. Compared to t-SNE, UMAP offers a more interpretable global view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51c3f1",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the quantitative features\n",
    "X_scaled = StandardScaler().fit_transform(df[quantitative_cols])\n",
    "\n",
    "# Apply PCA\n",
    "# Use full SVD solver (numerically more stable)\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca[target_col] = df[target_col].values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=target_col, palette=\"tab10\", s=10, alpha=0.7)\n",
    "plt.title(\"PCA - First 2 Principal Components\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend(title=\"Cover Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea504bf2",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress runtime warnings temporarily\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # Optionally disable NumPy warnings too\n",
    "    np.seterr(all=\"ignore\")\n",
    "\n",
    "    # Use fewer samples for performance if dataset is large\n",
    "    n_samples = 5000\n",
    "    X_sample = X_scaled[:n_samples]\n",
    "    y_sample = df[target_col].iloc[:n_samples]\n",
    "\n",
    "    # Run t-SNE with safe parameters\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=30,\n",
    "        learning_rate='auto',\n",
    "        max_iter=1000,\n",
    "        init='pca',\n",
    "        random_state=42\n",
    "    )\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "# Reset NumPy warning behavior (optional)\n",
    "np.seterr(all=\"warn\")\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_sample, palette=\"tab10\", s=10, alpha=0.7)\n",
    "plt.title(\"t-SNE Projection (2D)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"Cover Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4fb22",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*omp_set_nested.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*n_jobs value 1 overridden to 1.*\", category=UserWarning)\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=RANDOM_SEED)\n",
    "X_umap = umap_model.fit_transform(X_scaled[:5000])\n",
    "y_umap = df[\"Cover_Type\"].iloc[:5000]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y_umap, palette=\"tab10\", s=10, alpha=0.7)\n",
    "plt.title(\"UMAP Projection (2D)\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.legend(title=\"Cover Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40411b26",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031080a3",
   "metadata": {},
   "source": [
    "### 🧠 Model Selection Justification\n",
    "\n",
    "This dataset exhibits several characteristics that directly influence model effectiveness:\n",
    "\n",
    "- **Mixed data types**: It contains both **continuous features** (e.g., Elevation, Slope, Hillshade) and **high-dimensional binary features** (e.g., `Soil_TypeX`, `Wilderness_AreaX`).\n",
    "- **High cardinality in binary features**: The presence of 40 soil types benefits models that can handle **sparse, one-hot encoded inputs** effectively.\n",
    "- **Nonlinear relationships**: Variables such as `Elevation`, `Hillshade`, and `Distance to Hydrology` are likely to influence the target in a nonlinear fashion.\n",
    "- **Multiclass classification**: The target has **seven distinct forest cover types**, requiring models that support multiclass classification and can balance class-specific performance.\n",
    "- **Slight class imbalance**: Some classes appear more frequently than others, making metrics like **recall** and **G-Mean** important for evaluation.\n",
    "\n",
    "Given these properties, the following model types are most appropriate:\n",
    "\n",
    "#### ✅ Recommended:\n",
    "- **Tree-based models** (e.g., Random Forest, XGBoost): These models naturally handle mixed feature types, model nonlinear relationships, and are robust to class imbalance.\n",
    "- **K-Nearest Neighbors**: Performs reasonably well due to sensitivity to local feature patterns, though scalability may be a limitation for larger datasets.\n",
    "\n",
    "#### ⚠️ Less Suitable:\n",
    "- **Linear models** (e.g., Logistic Regression, Linear SVM): These are useful as interpretable baselines but often fail to capture the complexity of nonlinear, high-dimensional relationships.\n",
    "- **Naive Bayes**: Assumes independence among features, which is unrealistic in this dataset given environmental and spatial dependencies.\n",
    "- **Kernel-based SVMs**: Their high computational cost and poor performance in this case make them impractical without advanced tuning or dimensionality reduction.\n",
    "\n",
    "\n",
    "#### KPI\n",
    "Since the problem is imbalanced, accuracy can be misleading due to a bias toward the majority class. For this reason, we also report the F1 Score, Recall, Specificity and G-Mean, which are less sensitive to class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Model Performance Comparison and Insights\n",
    "\n",
    "The table below compares the performance of multiple ML models using several evaluation metrics: **Accuracy**, **F1 Score**, **G-Mean**, **Recall**, and **Specificity**. This helps assess both overall and per-class model behavior.\n",
    "\n",
    "#### 🔹 Top Performing Models:\n",
    "- **Random Forest** (both standard and balanced variants) leads in every metric, with over **95% accuracy and F1**, and excellent per-class balance.\n",
    "- **K-Nearest Neighbors** also performs strongly, achieving above **92% accuracy** and solid G-Mean and recall.\n",
    "- **XGBoost**, while slightly lower in raw accuracy (~87%), maintains **strong class balance**, with high G-Mean and decent recall.\n",
    "\n",
    "#### 🔸 Moderate Models:\n",
    "- **Logistic Regression** and **Linear SVMs** perform acceptably but struggle with **recall and G-Mean**, suggesting weaker performance on minority classes.\n",
    "- **Balanced variants** of these models help boost recall but typically reduce precision and overall F1.\n",
    "\n",
    "#### 🔻 Weak Performers:\n",
    "- **Standard and balanced SVM (non-linear kernels)** and **Naive Bayes** perform poorly, with very low F1 and G-Mean scores. This reflects their inability to manage the complexity and dimensionality of the dataset without significant preprocessing.\n",
    "\n",
    "#### 📝 Additional Observations:\n",
    "- The **use of class weighting (balanced versions)** had mixed effects. While it improved recall in underrepresented classes, it often **reduced overall accuracy and F1**, especially in models like SVM.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Conclusion\n",
    "\n",
    "Given the structure of the dataset, **Random Forest** and **XGBoost** are the most effective models — they handle the mix of continuous and binary features well, require minimal preprocessing, and yield high and balanced performance. Simpler models are useful for interpretability or benchmarking but are not suitable for production-level performance in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1be7b",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_quant = df[quantitative_cols]\n",
    "X_binary = df[binary_cols]\n",
    "y = df[target_col] - 1  # Convert labels from 1–7 to 0–6\n",
    "\n",
    "# Train-test split\n",
    "X_train_q, X_test_q, X_train_bin, X_test_bin, y_train, y_test = train_test_split(\n",
    "    X_quant, X_binary, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f672c",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale only the quantitative features\n",
    "scaler = StandardScaler()\n",
    "X_train_q_scaled = scaler.fit_transform(X_train_q)\n",
    "X_test_q_scaled = scaler.transform(X_test_q)\n",
    "\n",
    "# Concatenate scaled quantitative with binary features\n",
    "X_train = np.hstack([X_train_q_scaled, X_train_bin])\n",
    "X_test = np.hstack([X_test_q_scaled, X_test_bin])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5b95f",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=100),\n",
    "    \"Logistic Regression - Balanced\": LogisticRegression(max_iter=100, class_weight='balanced'),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED),\n",
    "    \"Random Forest - Balanced\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=RANDOM_SEED),\n",
    "\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"LinearSVC\": LinearSVC(max_iter=100),\n",
    "    \"LinearSVC - Balanced\": LinearSVC(max_iter=100, class_weight='balanced'),\n",
    "\n",
    "    \"Support Vector Machine\": SVC(max_iter=100),\n",
    "    \"Support Vector Machine - Balanced\": SVC(max_iter=100, class_weight='balanced'),\n",
    "\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', verbosity=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a6bb8",
   "metadata": {},
   "source": [
    "### Train and evaluate each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "trained_models = train_models(models, X_train, y_train)\n",
    "results = evaluate_models(trained_models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0082d",
   "metadata": {},
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8440dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\n",
    "print(\"\\n📊 Model Performance Comparison:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fff09",
   "metadata": {},
   "source": [
    "### Visualize model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca233dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=results_df, x=\"F1 Score\", y=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"F1 Score by Model\")\n",
    "plt.xlabel(\"F1 Score (weighted)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca67eeb",
   "metadata": {},
   "source": [
    "## Analysis of Normalized Confusion Matrix – Random Forest (Balanced)\n",
    "\n",
    "The normalized confusion matrix provides insight into how well the Random Forest model (the best ML model) performs on a per-class basis, accounting for class imbalance. Key observations:\n",
    "\n",
    "- **High accuracy for most classes**: The diagonal values are generally high (e.g., class 0: 94%, class 1: 97%, class 2: 96%, class 6: 95%), indicating strong model performance in correctly identifying those classes.\n",
    "- **Moderate confusion for class 4**: Class 4 has the lowest correct classification rate (79%) and is most often confused with class 1 (18%). This suggests overlap in feature space or underrepresentation during training.\n",
    "- **Minor misclassifications elsewhere**: Small values off the diagonal (mostly <5%) suggest occasional confusion between classes, but not substantial enough to raise concern.\n",
    "- **Balanced learning**: The model's performance does not heavily favor dominant classes, showing that balancing techniques likely helped mitigate bias toward more frequent labels.\n",
    "\n",
    "Overall, the Random Forest model demonstrates solid generalization across all classes, though targeted improvements for classes with lower accuracy (e.g., class 4) could further enhance performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1 Score\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Compute normalized confusion matrix (by true label)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best, normalize='true')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_best, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar=True,\n",
    "            xticklabels=np.unique(y_test),\n",
    "            yticklabels=np.unique(y_test))\n",
    "plt.title(f\"Normalized Confusion Matrix - {best_model_name}\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b36bb",
   "metadata": {},
   "source": [
    "## 🚀 Possible Next Steps to Improve the Project\n",
    "\n",
    "1. **Hyperparameter Optimization**  \n",
    "   Improve the performance and robustness of the machine learning models by implementing hyperparameter tuning using:\n",
    "   - `GridSearchCV` or `RandomizedSearchCV` from `scikit-learn`\n",
    "   - Cross-validation to ensure generalization and avoid overfitting\n",
    "\n",
    "2. **Feature Attribution and Interpretability**  \n",
    "   Apply feature attribution methods to gain insights into model decision-making:\n",
    "   - **SHAP (SHapley Additive exPlanations)** for model-agnostic interpretability\n",
    "   - **Permutation importance** for simple, intuitive feature ranking\n",
    "   - Consider **LIME** or **Integrated Gradients** for deep learning models\n",
    "\n",
    "These enhancements can lead to more accurate, explainable, and actionable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
